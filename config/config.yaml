# timeseries split
test_split_year: 2021
dev_start_year: 2018
interval: 6 # months

# column names
label_col: 'CTPE'
patient_col: 'patientid'
date_col: 'datetime'
text_col: 'text'

# model training arguments
# Ref: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments
training_args:
  num_train_epochs: 50             # total # of training epochs
  per_device_train_batch_size: 8   # batch size per device during training
  per_device_eval_batch_size: 8    # batch size per device during evaluation

  # save memory by using 8-bit Adam optimizer...didn't make much difference,
  # tho maybe bitsandbytes wasn't built properly
  # optim: adamw_bnb_8bit
  optim: adamw_torch

  # save memory by imitating large batch sizes 
  # (e.g. batch size x gradient accumulation step = effective batch size)
  gradient_accumulation_steps: 2   # number of times to accumulate gradients before updating weights

  # save memory (at the expense of slower computation) by storing only select activations from forward 
  # pass and recompute the unstored activations during backward pass
  # gradient_checkpointing: true

  # save memory (and speed up computation) by using floating point 16-bit precision instead of 32-bit
  fp16: true

  # save memory by using deepspeed (ZeRO optimization)
  # Ref: https://huggingface.co/docs/transformers/main_classes/deepspeed
  # deepspeed: ./config/ds_config_zero3.json

  load_best_model_at_end: true
  metric_for_best_model: AUROC
  save_strategy: epoch             # save model checkpoint at end of each epoch
  save_total_limit: 1              # max number of model checkpoints to keep on disk

  evaluation_strategy: epoch       # evaluate at end of each epoch
  logging_strategy: epoch          # log at end of each epoch

  output_dir: ./results            # output directory
  overwrite_output_dir: true
  seed: 42
early_stopping_patience: 5